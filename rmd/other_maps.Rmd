---
title: "Calculating Optimal Placement Locations"
output:
  html_document:
    toc: yes
  html_notebook:
    highlight: zenburn
    toc: yes
---

```{r setup, include=FALSE}
require("knitr")
library(rprojroot)
opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
```



```{r}
source('code/00_dependencies.R')
# source("code/02_load_placement_datasets.R")

incidents <- read_feather('data/source_incidents/incidents.feather')

##Use 2016 calls less February
spatial_call_data <- incidents %>%
	filter(!is.na(Latitude), 
				 !is.na(Longtitude), 
				 !is.na(ArrivedTime),
				  year(IncidentDate) == 2016 & 
          month(IncidentDate) != 2) %>% 
	mutate( DayShift = if_else( between(hour(IncidentDate),7,18),"Day","Night"),
					DayShift = factor(DayShift)) %>%
  spatialize_incidents()
```




# Setup  

## Drawing polygons  

- An ArcGIS Online Map with candidate locations can be used to draw drive time polygons.  It's found on AGO in abwilliamson folder... called EMS_candidate_location_map.  It uses points found in this project directory: data/input_points/cleaned_points.  High traffic = 5pm Monday.  Low-traffic = 2am Tuesday

- Draw the polygons, extract the shapefile to data/algorithm/polygons/


## Load polygons  

```{r}
NOLA.proj <- CRS("+init=epsg:3452")

polygons_directory <- "data/drive_time_polygons"
layer_name <- "Drive_Time_polygons___8_Minutes"

polygons <- readOGR(polygons_directory, layer = layer_name)
  
polygons <- polygons[ which( !grepl("martin luther king", 
																		polygons@data$Address, 
																		ignore.case = TRUE  )),] %>%
	spTransform( NOLA.proj)

polygons$NewName <- as.character(polygons$NewName)
```


## Select calls for high traffic/DayShift   
```{r select_calls}
input_calls <- spatial_call_data[ spatial_call_data@data$DayShift == 'Day' &  
                             date(spatial_call_data@data$IncidentDate) >= date('2016-06-01') &
                             spatial_call_data$InitialPriority == 3,] %>% 
  spTransform( NOLA.proj) 
```

# Finding optimal placement  

## Determine "base set" of polygons  


### Coverage Estimates  

For every set of `solution_size` polygons, count how many calls are within 8-minute
driving distance.

```{r estimate_coverage}
traffic <- 'high'

solution_size <- 2

additional_description <- 'example_quick_execution'

coverage_estimates_file_name <- paste0( "data/", today(), 
																				"_coverage_estimates_high_traffic", 
																				additional_description,"_traffic_", 
																				solution_size, ".csv")

##For performance, run with a sample.  For full calculation, run with all drive-time polygons.
polygon_sample <- polygons[1:10,]

coverage_estimates <- solve_simultaneous(input_calls, polygon_sample, solution_size) %>%
	write_csv(coverage_estimates_file_name)
```


### Define Top Coverage Tier  


Now look at the coverage offered by these solution sets.  A good way to compare coverage across solution sets by performing a Poisson Test comparison with the highest-level of coverage.  The resulting p-value represents the probability of seeing a coverage rate less than or equal to the observed coverage rate, given that the actual coverage rate was the same as the top coverage rate.  


```{r poisson_test}
coverage_estimates <- add_poisson_test(coverage_estimates)

p_value_threshold <- .0000001

top_tier_polygon_sets <- coverage_estimates %>% 
	filter(p.value >= p_value_threshold) %>%
  mutate(p.value = round(p.value,4))
```


### Calculate Overlapping coverage  

How many calls are covered by more than one drive-time polygon?  

```{r overlap_calc}
ptm <- proc.time()


polygon_names     <- top_tier_polygon_sets[, grepl("V",names( top_tier_polygon_sets))]

output  <- top_tier_polygon_sets %>% 
  mutate( indic = 1, 
  				ID = cumsum(indic)) %>% 
  select(-indic) %>%
  group_by(ID) %>%
  nest() %>%
  mutate(location_names = map(data, ~.x[, grepl("V",names( .x))]),
  			 polygon_set = 
						map(location_names,~polygons[polygons$NewName %in%  unlist(.x), ]),
					coverage = 
						map(polygon_set, calculate_multi_coverage, input_calls))




print(proc.time() - ptm)

```



```{r simplify_overlap}
overlap_measurements <- output %>% 
  unnest(data, coverage, .drop = TRUE) %>% 
  mutate(TotalCalls = length(input_calls)) %>% 
  mutate(CoveragePercent = percent(coverage/TotalCalls),
          DoubleCoveragePercent = percent(double_coverage/TotalCalls),
            TripleCoveragePercent = percent(triple_coverage/TotalCalls),
         coverage_sum = double_coverage + triple_coverage) %>% 
  arrange(desc(coverage_sum))
```



### Selecting Optimal Set  

Ultimately, the top solution is determined by a balance of coverage measurements, and usability as determined by user-testing with the dispatch team.

```{r select_optimal_set}
optimal_index <- 1

solution_list <- overlap_measurements

solution <- solution_list[optimal_index,]

top_polygon_names <- solution[, grepl("V",names(solution_list))]

top_polygons <- polygons[polygons$NewName %in%  unlist(top_polygon_names), ]

top_polygons_data <- top_polygons@data

top_polygons_data <- top_polygons_data %>%
	dplyr::select(c(IsNew:Address))
```



## Filling out the rest of the list  

Now that the first `solution size` polygons have been determined, order them and calulate the remaining points.  


### Ordering initial set  

```{r}
ordered_polygons <- order_by_coverage(input_calls, top_polygons)

ordered_solution <- arrange(ordered_polygons@data, desc(CallCount))
```


### Determining the rest  
```{r}
full_list <- rank.polygon.impact(input_calls, polygons, rank.to = 10, seed.polygons = top_polygons)

full_list_no_seed <- rank.polygon.impact(input_calls, polygons, rank.to = 10)

```


### Clean up  

```{r}
full_list_data <- full_list@data %>%
	mutate(CumulativeCoverage = percent(cumsum(CallCount)/nrow(input_calls@data)),
				 ReplaceIfVacant = TRUE)

full_list_data$ReplaceIfVacant[1:3] <- TRUE
full_list_data$ReplaceIfVacant[-(1:3)] <- FALSE


full_list_data$Order <- row(full_list_data)[,1]
```



```{r}
all_points <- readOGR("data/input_points", layer = "cleaned_points") %>% 
  spTransform( CRS("+init=epsg:4326"))

all_points_data <- all_points@data %>% 
  mutate(NewName = as.character(NewName))  

calculated_point_order <- full_list_data %>% 
  select(NewName, Order, ReplaceIfVacant)

updated_data <- all_points_data %>% 
  left_join(calculated_point_order, by = c( "NewName"))%>%
  mutate( ReplaceIfVacant = ifelse(ReplaceIfVacant == TRUE,"X",NA)) %>%
  mutate(Address = as.character(Address),
         NewName = as.character(NewName))

all_points@data <- updated_data
```


```{r}
all_points_df <- data.frame(all_points)
```


```{r}
solution_names <- all_points@data %>% 
	filter(is.na(Order) == FALSE)

solution_points <- all_points[all_points$NewName %in% solution_names$NewName,] 

solution_points_df <- data.frame(solution_points) %>% 
	arrange(Order) %>% 
	slice(1:8)
```





### Solution Map  

```{r}
map <- qmap("New Orleans", zoom = 11, maptype = "roadmap") +
  geom_point(data = solution_points_df, aes(coords.x1, coords.x2)) +
  guides(size="none") +
  geom_label(size = 4, 
            data = solution_points_df, 
            aes(x=coords.x1, y=coords.x2, label=Order,hjust=0,vjust=0), position = "nudge",
            fontface = "bold",
            check_overlap = TRUE
            )
```

```{r}
print_list <- select(solution_points_df,c(NewName,Address,ReplaceIfVacant,Order)) 

names(print_list) <- c("Name", "Address","Primary Location","Rank")
```



```{r}
map
```



```{r}
print_list
```



```{r}
library(ggfortify)
make.graphable <- function(x) {
  graphables <- spTransform(x, CRS("+init=epsg:4326"))
  graphables@data$labX <- as.numeric(lapply(graphables@polygons, function(x) x@labpt[[1]]))
  graphables@data$labY <- as.numeric(lapply(graphables@polygons, function(x) x@labpt[[2]]))
  graphables@data$id <- rownames(graphables@data)
  graphables.points <- fortify(graphables, region="id")
  join(graphables.points, graphables@data, by="id")
}

names_to_graph <- c('Tulane Medical Center','SQUIRT 14',
                    'Westbank Tolls',
                    # 'I-10 and Read Blvd',
                    'I-10 and Morrison',
                    # "Old Gentilly/5000", 
                    "I-10 and Downman Rd")
# polygons.to.graph <- make.graphable(polygons[polygons$NewName %in% names_to_graph,])
  polygons.to.graph <- make.graphable(full_list[1:4,])                           
qmap("New Orleans", zoom = 11, maptype = "roadmap") +
  geom_polygon(aes_string(x = "long", y = "lat", 
                          group = 'group', fill = 'NewName'
                          ), 
               data = polygons.to.graph, 
               alpha = 0.5, 
               color = "black"
               ) 


  polygons.to.graph <- make.graphable(full_list[5:8,])                           
qmap("New Orleans", zoom = 11, maptype = "roadmap") +
  geom_polygon(aes_string(x = "long", y = "lat", 
                          group = 'group', fill = 'NewName'
                          ), 
               data = polygons.to.graph, 
               alpha = 0.5, 
               color = "black"
               ) 
```



## Save list  

```{r}
write_csv( solution_points_df,  paste("data/algorithm/", today(), "_coverage_estimates_",additional_description,traffic,"_traffic_", 
                                      solution_size,".csv",
                              sep = ""))
```
